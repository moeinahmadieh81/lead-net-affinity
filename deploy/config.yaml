namespaceSelector: ["default"]

graph:
  entry: frontend
  services:
    - name: frontend
      dependsOn: [search, user, recommendation, reservation]

    - name: search
      dependsOn: [profile, geo, rate]

    - name: profile
      dependsOn: [memcached-profile, mongodb-profile]

    - name: geo
      dependsOn: [mongodb-geo]

    - name: rate
      dependsOn: [memcached-rate, mongodb-rate]

    - name: user
      dependsOn: [mongodb-user]

    - name: recommendation
      dependsOn: [mongodb-recommendation]

    - name: reservation
      dependsOn: [memcached-reserve, mongodb-reservation]

    - name: memcached-profile
      dependsOn: []

    - name: mongodb-profile
      dependsOn: []

    - name: mongodb-geo
      dependsOn: []

    - name: memcached-rate
      dependsOn: []

    - name: mongodb-rate
      dependsOn: []

    - name: mongodb-user
      dependsOn: []

    - name: mongodb-recommendation
      dependsOn: []

    - name: memcached-reserve
      dependsOn: []

    - name: mongodb-reservation
      dependsOn: []

prometheus:
  url: "http://prometheus-kube-prometheus-prometheus.monitoring:9090"

  nodeRTTQuery: |
    histogram_quantile(
      0.5,
      sum(
        rate(cilium_node_health_connectivity_latency_seconds_bucket[10m])
      ) by (instance, le)
    )

  nodeDropRateQuery: |
    sum(
      rate(cilium_drop_bytes_total[10m])
    ) by (instance)

  nodeBandwidthQuery: |
    sum(
      rate(hubble_flows_processed_total[10m])
    ) by (instance)

  sampleWindow: "10m"

scoring:
  #
  # Base LEAD scoring (unchanged)
  #
  pathLengthWeight:     1.0
  podCountWeight:       1.0
  serviceEdgesWeight:   1.0
  rpsWeight:            0.0

  #
  # Per-node severity weights:
  # How much each dimension should influence placement.
  #
  netLatencyWeight:     2.0      # high latency should strongly penalize
  netDropWeight:        3.0      # drops are severe
  netBandwidthWeight:   1.0      # higher flow rate = busier node

  #
  # Interpretation thresholds for severity normalization:
  #
  badLatencyMs:   100.0   # ~100ms considered "bad" for nano-services
  badDropRate:    10000   # depends on real traffic
  badBandwidthRate: 20.0  # nodes above ~20 flows/sec are "busy"

affinity:
  #
  # How many top paths affect pod affinity
  #
  topPaths:           5

  #
  # Kubernetes affinity weights turned into 1â€“100 range
  #
  minAffinityWeight:  50
  maxAffinityWeight:  100
